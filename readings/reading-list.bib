@article{Turney:2010aa,
  author =        {Turney, Peter D and Pantel, Patrick and others},
  journal =       {Journal of artificial intelligence research},
  number =        {1},
  pages =         {141--188},
  title =         {From frequency to meaning: Vector space models of
                   semantics},
  volume =        {37},
  year =          {2010},
  doi =           {http://dx.doi.org/10.1613/jair.2934},
}

@article{Talman:2019aa,
  author =        {Talman, Aarne and Yli-Jyr{\"a}, Anssi and
                   Tiedemann, J{\"o}rg},
  journal =       {Natural Language Engineering},
  number =        {4},
  pages =         {467--482},
  publisher =     {Cambridge University Press},
  title =         {Sentence embeddings in NLI with iterative refinement
                   encoders},
  volume =        {25},
  year =          {2019},
  doi =           {10.1017/S1351324919000202},
  url =           {https://doi.org/10.1017/S1351324919000202},
}

@incollection{Stone:2014aa,
  address =       {Cambridge, UK},
  author =        {Stone, Matthew},
  booktitle =     {The Cambridge Handbook of Formal Semantics},
  chapter =       {25},
  editor =        {Maria Aloni and Paul Dekker},
  journal =       {The Cambridge Handbook of Formal Semantics
                   (forthcoming)},
  month =         {July},
  pages =         {775--800},
  publisher =     {Cambridge University Press},
  series =        {Cambridge Handbooks in Language and Linguistics},
  title =         {Semantics and computation},
  year =          {2016},
}

@inproceedings{Silberer:2014aa,
  address =       {Baltimore, Maryland, USA},
  author =        {Carina Silberer and Mirella Lapata},
  booktitle =     {Proceedings of the 52nd Annual Meeting of the
                   Association for Computational Linguistics},
  month =         {June 23--25 2014},
  organization =  {Association for Computational Linguistics},
  pages =         {721--732},
  title =         {Learning Grounded Meaning Representations with
                   Autoencoders},
  year =          {2014},
}

@article{Roy:2005,
  address =       {Essex, UK},
  author =        {Roy, Deb},
  journal =       {Artificial Intelligence},
  month =         {September},
  number =        {1-2},
  pages =         {170--205},
  publisher =     {Elsevier Science Publishers Ltd.},
  title =         {Semiotic schemas: a framework for grounding language
                   in action and perception},
  volume =        {167},
  year =          {2005},
  doi =           {10.1016/j.artint.2005.04.007},
  issn =          {0004-3702},
}

@techreport{Rehurek:2019aa,
  author =        {{\v R}eh{\r u}{\v r}ek, Radim},
  institution =   {Gensim},
  month =         {November},
  title =         {models.word2vec -- {Word2vec} embeddings},
  year =          {2019},
  url =           {https://radimrehurek.com/gensim/models/word2vec.html},
}

@inproceedings{Rehurek:2010aa,
  address =       {Valletta, Malta},
  author =        {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  booktitle =     {Proceedings of the LREC 2010 Workshop on New
                   Challenges for NLP Frameworks},
  month =         may,
  pages =         {45--50},
  publisher =     {ELRA},
  title =         {Software Framework for Topic Modelling with Large
                   Corpora},
  year =          {2010},
  language =      {English},
  url =           {http://is.muni.cz/publication/884893/en},
}

@techreport{Pulman:2005ab,
  address =       {Oxford, United Kingdom},
  author =        {Pulman, Stephen G.},
  institution =   {Department of Computer Science, University of Oxford},
  type =          {lecture notes},
  title =         {Higher Order Logic in Semantics},
  year =          {2005},
}

@inproceedings{Peters:2018aa,
  address =       {New Orleans, Louisiana},
  author =        {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and
                   Gardner, Matt and Clark, Christopher and Lee, Kenton and
                   Zettlemoyer, Luke},
  booktitle =     {Proceedings of the 2018 Conference of the North
                   American Chapter of the Association for Computational
                   Linguistics: Human Language Technologies, Volume 1
                   (Long Papers)},
  month =         jun,
  pages =         {2227--2237},
  publisher =     {Association for Computational Linguistics},
  title =         {Deep Contextualized Word Representations},
  year =          {2018},
  doi =           {10.18653/v1/N18-1202},
  url =           {https://www.aclweb.org/anthology/N18-1202},
}

@inproceedings{Pennington:2014aa,
  author =        {Pennington, Jeffrey and Socher, Richard and
                   Manning, Christopher},
  booktitle =     {Proceedings of the 2014 conference on empirical
                   methods in natural language processing (EMNLP)},
  pages =         {1532--1543},
  title =         {Glove: Global vectors for word representation},
  year =          {2014},
  url =           {http://www.aclweb.org/anthology/D14-1162},
}

@techreport{Olah:2015aa,
  author =        {Olah, Christopher},
  institution =   {Google Brain},
  month =         {August 27},
  title =         {Understanding {LSTM}s},
  year =          {2015},
  url =           {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
}

@techreport{Nivre:2015aa,
  address =       {December 8},
  author =        {Joakim Nivre},
  institution =   {Uppsala University},
  type =          {Lecture slides (adapted from Dan Jurafsky and James
                   Martin)},
  title =         {Word Senses},
  year =          {2015},
  url =           {https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf},
}

@book{Montague1974,
  address =       {New Haven},
  author =        {Montague, Richard},
  note =          {ed. and with an introduction by Richmond H. Thomason},
  publisher =     {Yale University Press},
  title =         {Formal Philosophy: Selected Papers of Richard
                   Montague},
  year =          {1974},
}

@article{Mitchell:2010aa,
  author =        {Mitchell, Jeff and Lapata, Mirella},
  journal =       {Cognitive Science},
  number =        {8},
  pages =         {1388--1429},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Composition in Distributional Models of Semantics},
  volume =        {34},
  year =          {2010},
  doi =           {10.1111/j.1551-6709.2010.01106.x},
  issn =          {1551-6709},
  url =           {http://dx.doi.org/10.1111/j.1551-6709.2010.01106.x},
}

@inproceedings{Mitchell:2008uq,
  address =       {Columbus, Ohio},
  author =        {Mitchell, Jeff and Lapata, Mirella},
  booktitle =     {Proceedings of ACL-08: HLT},
  pages =         {236--244},
  title =         {Vector-based Models of Semantic Composition},
  year =          {2008},
  url =           {https://www.aclweb.org/anthology/P08-1028/},
}

@inproceedings{Mikolov:2013ab,
  author =        {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and
                   Corrado, Greg S and Dean, Jeff},
  booktitle =     {Advances in neural information processing systems},
  pages =         {3111--3119},
  title =         {Distributed representations of words and phrases and
                   their compositionality},
  year =          {2013},
  url =           {http://papers.nips.cc/paper/5021-distributed-
                  representations-of-words-andphrases},
}

@unpublished{Mikolov:2013aa,
  author =        {Mikolov, Tomas and Chen, Kai and Corrado, Greg and
                   Dean, Jeffrey},
  note =          {arXiv preprint arXiv:1301.3781},
  title =         {Efficient estimation of word representations in
                   vector space},
  year =          {2013},
}

@techreport{Manning:2017ab,
  author =        {Manning, Christopher D.},
  institution =   {Stanford University},
  type =          {Lecture notes: CS224N Natural Language Processing},
  title =         {Computational semantics},
  year =          {2017},
  url =           {https://web.stanford.edu/class/archive/cs/cs224n/
                  cs224n.1162/handouts/Computational-Semantics.pdf},
}

@techreport{Manning:2017aa,
  address =       {Simons Institute, Berkeley},
  author =        {Christopher Manning},
  institution =   {Stanford University},
  month =         {March 27th},
  type =          {talk},
  title =         {Representations for Language: From Word Embeddings to
                   Sentence Meanings},
  year =          {2017},
  url =           {https://simons.berkeley.edu/talks/christopher-manning-2017-
                  3-27},
}

@unpublished{Manning:2005aa,
  author =        {Christopher D. Manning},
  note =          {Lecture notes for CS224N/Ling 280},
  title =         {An Introduction to Formal Computational Semantics},
  year =          {2005},
  url =           {https://prod-c2g.s3.amazonaws.com/cs224n/Fall2012/files/cl-
                  semantics-new.pdf},
}

@article{Levy:2015aa,
  author =        {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {211--225},
  title =         {Improving Distributional Similarity with Lessons
                   Learned from Word Embeddings},
  volume =        {3},
  year =          {2015},
  abstract =      {Recent trends suggest that neural-network-inspired
                   word embedding models outperform traditional
                   count-based distributional models on word similarity
                   and analogy detection tasks. We reveal that much of
                   the performance gains of word embeddings are due to
                   certain system design choices and hyperparameter
                   optimizations, rather than the embedding algorithms
                   themselves. Furthermore, we show that these
                   modifications can be transferred to traditional
                   distributional models, yielding similar gains. In
                   contrast to prior reports, we observe mostly local or
                   insignificant performance differences between the
                   methods, with no global advantage to any single
                   approach over the others.},
  issn =          {2307-387X},
  url =           {https://transacl.org/ojs/index.php/tacl/article/view/570},
}

@book{Lappin:2015aa,
  booktitle =     {The Handbook of Contemporary Semantic Theory},
  edition =       {2nd},
  editor =        {Lappin, Shalom and Fox, Chris},
  publisher =     {Wiley-Blackwell},
  title =         {The Handbook of Contemporary Semantic Theory},
  year =          {2015},
  isbn =          {9780470670736 (cloth)},
}

@inproceedings{Kageback:2016aa,
  author =        {K{\aa}geb{\"a}ck, Mikael and Salomonsson, Hans},
  booktitle =     {5th Workshop on Cognitive Aspects of the Lexicon
                   (CogALex)},
  organization =  {Association for Computational Linguistics},
  title =         {Word Sense Disambiguation using a Bidirectional LSTM},
  year =          {2016},
  abstract =      {In this paper we present a clean, yet effective,
                   model for word sense disambiguation. Our approach
                   leverage a bidirectional long short-term memory
                   network which is shared between all words. This
                   enables the model to share statistical strength and
                   to scale well with vocabulary size. The model is
                   trained end-to-end, directly from the raw text to
                   sense labels, and makes effective use of word order.
                   We evaluate our approach on two standard datasets,
                   using identical hyperparameter settings, which are in
                   turn tuned on a third set of held out data. We employ
                   no external resources (e.g. knowledge graphs,
                   part-of-speech tagging, etc), language specific
                   features, or hand crafted rules, but still achieve
                   statistically equivalent results to the best
                   state-of-the-art systems, that employ no such
                   limitations.},
  url =           {http://www.cse.chalmers.se/~kageback/word-sense-
                  disambiguation-using-a-bidirectional-lstm/},
}

@techreport{Jurafsky:2019aa,
  author =        {Jurafsky, Dan and Martin, James H.},
  institution =   {Stanford University and University of Colorado at
                   Boulder},
  month =         {October 16},
  type =          {Third edition draft},
  title =         {Speech and language processing: an introduction to
                   natural language processing, computational
                   linguistics, and speech recognition},
  year =          {2019},
  url =           {https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf},
}

@article{Hill:2015aa,
  author =        {Hill, Felix and Reichart, Roi and Korhonen, Anna},
  journal =       {Computational Linguistics},
  number =        {4},
  pages =         {665-695},
  title =         {SimLex-999: Evaluating Semantic Models With (Genuine)
                   Similarity Estimation},
  volume =        {41},
  year =          {2015},
  abstract =      {We present SimLex-999, a gold standard resource for
                   evaluating distributional semantic models that
                   improves on existing resources in several important
                   ways. First, in contrast to gold standards such as
                   WordSim-353 and MEN, it explicitly quantifies
                   similarity rather than association or relatedness so
                   that pairs of entities that are associated but not
                   actually similar (Freud, psychology) have a low
                   rating. We show that, via this focus on similarity,
                   SimLex-999 incentivizes the development of models
                   with a different, and arguably wider, range of
                   applications than those which reflect conceptual
                   association. Second, SimLex-999 contains a range of
                   concrete and abstract adjective, noun, and verb
                   pairs, together with an independent rating of
                   concreteness and (free) association strength for each
                   pair. This diversity enables fine-grained analyses of
                   the performance of models on concepts of different
                   types, and consequently greater insight into how
                   architectures can be improved. Further, unlike
                   existing gold standard evaluations, for which
                   automatic approaches have reached or surpassed the
                   inter-annotator agreement ceiling, state-of-the-art
                   models perform well below this ceiling on SimLex-999.
                   There is therefore plenty of scope for SimLex-999 to
                   quantify future improvements to distributional
                   semantic models, guiding the development of the next
                   generation of representation-learning architectures.},
  doi =           {10.1162/COLI\_a\_00237},
  url =           {https://doi.org/10.1162/COLI_a_00237},
}

@techreport{Hewitt:2017aa,
  author =        {Steven Hewitt},
  institution =   {O'Reilly and TensorFlow},
  month =         {July},
  type =          {Tutorial and code},
  title =         {Textual entailment with TensorFlow: Using neural
                   networks to explore natural language},
  year =          {2017},
  url =           {https://www.oreilly.com/content/textual-entailment-with-
                  tensorflow/},
}

@article{Harris:1954aa,
  author =        {Zellig S. Harris},
  journal =       {Word},
  number =        {2-3},
  pages =         {146--162},
  title =         {Distributional Structure},
  volume =        {10},
  year =          {1954},
  doi =           {10.1080/00437956.1954.11659520},
  url =           {http://dx.doi.org/10.1080/00437956.1954.11659520},
}

@article{Harnad:1990,
  author =        {Harnad, Stevan},
  journal =       {Physica D},
  month =         {June},
  number =        {1--3},
  pages =         {335--346},
  title =         {The symbol grounding problem},
  volume =        {42},
  year =          {1990},
  doi =           {10.1016/0167-2789(90)90087-6},
}

@article{Goodman:2016aa,
  author =        {Goodman, Noah D. and Frank, Michael C.},
  journal =       {Trends in Cognitive Sciences},
  number =        {11},
  pages =         {818--829},
  publisher =     {Elsevier},
  title =         {Pragmatic Language Interpretation as Probabilistic
                   Inference},
  volume =        {20},
  year =          {2016},
  abstract =      {Understanding language requires more than the use of
                   fixed conventions and more than decoding
                   combinatorial structure. Instead, comprehenders make
                   exquisitely sensitive inferences about what
                   utterances mean given their knowledge of the speaker,
                   language, and context. Building on developments in
                   game theory and probabilistic modeling, we describe
                   the rational speech act (RSA) framework for pragmatic
                   reasoning. RSA models provide a principled way to
                   formalize inferences about meaning in context; they
                   have been used to make successful quantitative
                   predictions about human behavior in a variety of
                   different tasks and situations, and they explain why
                   complex phenomena, such as hyperbole and vagueness,
                   occur. More generally, they provide a computational
                   framework for integrating linguistic structure, world
                   knowledge, and context in pragmatic language
                   understanding.},
  annote =        {doi: 10.1016/j.tics.2016.08.005},
  doi =           {10.1016/j.tics.2016.08.005},
  isbn =          {1364-6613},
  url =           {https://doi.org/10.1016/j.tics.2016.08.005},
}

@inproceedings{Ghanimifard:2017ab,
  address =       {Montpellier, France},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of {IWCS 2017}: 12th International
                   Conference on Computational Semantics},
  editor =        {Gardent, Claire and Retor\'{e}, Christian},
  month =         {September 19--22},
  pages =         {1--12},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning to Compose Spatial Relations with Grounded
                   Neural Language Models},
  year =          {2017},
  abstract =      {Language is compositional: we can generate and
                   interpret novel sentences by having a notion of the
                   meaning of their individual parts. Spatial
                   descriptions are grounded in perceptional
                   representations but their meaning is also defined by
                   what neighbouring words they co-occur with. In this
                   paper, we examine how language models conditioned on
                   perceptual features can capture the semantics of
                   composed phrases as well as of individual words. We
                   generate a synthetic dataset of spatial descriptions
                   referring to perceptual scenes and examine how
                   grounded language models built with deep neural
                   networks can account for compositionality of
                   descriptions -- by evaluating how the learned
                   language models can deal with novel grounded composed
                   descriptions and novel grounded decomposed
                   descriptions, constituents previously not seen in
                   isolation.},
  url =           {https://gup.ub.gu.se/publication/257763?lang=en},
}

@article{Firth:1957aa,
  author =        {Firth, John R.},
  journal =       {Studies in linguistic analysis},
  pages =         {1--32},
  title =         {A synopsis of linguistic theory 1930--1955},
  year =          {1957},
}

@article{Erk:2012aa,
  author =        {Erk, Katrin},
  journal =       {Language and Linguistics Compass},
  number =        {10},
  pages =         {635--653},
  publisher =     {Blackwell Publishing Ltd},
  title =         {Vector Space Models of Word Meaning and Phrase
                   Meaning: A Survey},
  volume =        {6},
  year =          {2012},
  abstract =      {Distributional models represent a word through the
                   contexts in which it has been observed. They can be
                   used to predict similarity in meaning, based on the
                   distributional hypothesis, which states that two
                   words that occur in similar contexts tend to have
                   similar meanings. Distributional approaches are often
                   implemented in vector space models. They represent a
                   word as a point in high-dimensional space, where each
                   dimension stands for a context item, and a word's
                   coordinates represent its context counts. Occurrence
                   in similar contexts then means proximity in space. In
                   this survey we look at the use of vector space models
                   to describe the meaning of words and phrases: the
                   phenomena that vector space models address, and the
                   techniques that they use to do so. Many word meaning
                   phenomena can be described in terms of semantic
                   similarity: synonymy, priming, categorization, and
                   the typicality of a predicate's arguments. But vector
                   space models can do more than just predict semantic
                   similarity. They are a very flexible tool, because
                   they can make use of all of linear algebra, with all
                   its data structures and operations. The dimensions of
                   a vector space can stand for many things: context
                   words, or non-linguistic context like images, or
                   properties of a concept. And vector space models can
                   use matrices or higher-order arrays instead of
                   vectors for representing more complex relationships.
                   Polysemy is a tough problem for distributional
                   approaches, as a representation that is learned from
                   all of a word's contexts will conflate the different
                   senses of the word. It can be addressed, using either
                   clustering or vector combination techniques. Finally,
                   we look at vector space models for phrases, which are
                   usually constructed by combining word vectors. Vector
                   space models for phrases can predict phrase
                   similarity, and some argue that they can form the
                   basis for a general-purpose representation framework
                   for natural language semantics.},
  doi =           {10.1002/lnco.362},
  issn =          {1749-818X},
  url =           {http://dx.doi.org/10.1002/lnco.362},
}

@book{Eijck:2010aa,
  address =       {Cambridge},
  author =        {Eijck, J. van and Unger, Christina},
  publisher =     {Cambridge University Press},
  title =         {Computational semantics with functional programming},
  year =          {2010},
  isbn =          {9780521760300},
}

@article{Devlin:2018aa,
  author =        {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and
                   Kristina Toutanova},
  journal =       {arXiv},
  pages =         {1--14},
  title =         {{BERT:} Pre-training of Deep Bidirectional
                   Transformers for Language Understanding},
  volume =        {arXiv:1810.04805 [cs.CL]},
  year =          {2018},
  url =           {http://arxiv.org/abs/1810.04805},
}

@article{Conneau:2017aa,
  author =        {Alexis Conneau and Douwe Kiela and Holger Schwenk and
                   Lo{\"{\i}}c Barrault and Antoine Bordes},
  journal =       {arXiv},
  pages =         {1--12},
  title =         {Supervised Learning of Universal Sentence
                   Representations from Natural Language Inference Data},
  volume =        {arXiv:1705.02364 [cs.CL]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1705.02364},
}

@incollection{Clark:2015aa,
  author =        {Clark, Stephen},
  booktitle =     {Handbook of Contemporary Semantics --- second
                   edition},
  chapter =       {16},
  editor =        {Lappin, Shalom and Fox, Chris},
  pages =         {493--522},
  publisher =     {Wiley -- Blackwell},
  title =         {Vector Space Models of Lexical Meaning},
  year =          {2015},
}

@book{Chierchia:2000uq,
  address =       {Cambridge, Mass},
  author =        {Chierchia, Gennaro and McConnell-Ginet, Sally},
  edition =       {2},
  publisher =     {MIT Press},
  title =         {Meaning and grammar: an introduction to semantics},
  year =          {2000},
  isbn =          {0262032694},
}

@inproceedings{Bowman:2019aa,
  address =       {Minneapolis, Minnesota},
  author =        {Bowman, Samuel and Zhu, Xiaodan},
  booktitle =     {Proceedings of the 2019 Conference of the North
                   {A}merican Chapter of the Association for
                   Computational Linguistics: Tutorials},
  month =         jun,
  pages =         {6--8},
  publisher =     {Association for Computational Linguistics},
  title =         {Deep Learning for Natural Language Inference},
  year =          {2019},
  abstract =      {This tutorial discusses cutting-edge research on NLI,
                   including recent advance on dataset development,
                   cutting-edge deep learning models, and highlights
                   from recent research on using NLI to understand
                   capabilities and limits of deep learning models for
                   language understanding and reasoning.},
  url =           {https://www.aclweb.org/anthology/N19-5002},
}

@inproceedings{Bowman:2016aa,
  address =       {Berlin, Germany},
  author =        {Bowman, Samuel R. and Gauthier, Jon and
                   Rastogi, Abhinav and Gupta, Raghav and
                   Manning, Christopher D. and Potts, Christopher},
  booktitle =     {Proceedings of the 54th Annual Meeting of the
                   Association for Computational Linguistics (Volume 1:
                   Long Papers)},
  month =         aug,
  pages =         {1466--1477},
  publisher =     {Association for Computational Linguistics},
  title =         {A Fast Unified Model for Parsing and Sentence
                   Understanding},
  year =          {2016},
  doi =           {10.18653/v1/P16-1139},
  url =           {https://www.aclweb.org/anthology/P16-1139},
}

@inproceedings{Bowman:2015ac,
  author =        {Bowman, Samuel R. and Angeli, Gabor and
                   Potts, Christopher and Manning, Christopher D.},
  booktitle =     {Proceedings of the 2015 Conference on Empirical
                   Methods in Natural Language Processing (EMNLP)},
  publisher =     {Association for Computational Linguistics},
  title =         {A large annotated corpus for learning natural
                   language inference},
  year =          {2015},
  url =           {https://www.aclweb.org/anthology/D15-1075/},
}

@book{BlackburnBos:2005,
  author =        {Blackburn, Patrick and Bos, Johan},
  publisher =     {CSLI Publications},
  title =         {Representation and inference for natural language.
                   {A} first course in computational semantics},
  year =          {2005},
  url =           {http://www.coli.uni-saarland.de/publikationen/softcopies/
                  Blackburn:1997:RIN.pdf},
}

@book{Bird:2009aa,
  author =        {Bird, Steven and Klein, Ewan and Loper, Edward},
  publisher =     {O'Reilly},
  title =         {Natural language processing with {P}ython},
  year =          {2009},
  abstract =      {This is an introduction to natural language
                   processing, which supports a variety of language
                   technologies, from predictive text and email
                   filtering to automatic summarization and translation},
  isbn =          {9780596516499},
  url =           {http://nltk.org/book/},
}

@article{Bengio:2003aa,
  author =        {Bengio, Yoshua and Ducharme, R{\'e}jean and
                   Vincent, Pascal and Janvin, Christian},
  journal =       {Journal of Machine Learning Research},
  number =        {6},
  pages =         {1137--1155},
  title =         {A Neural Probabilistic Language Model},
  volume =        {3},
  year =          {2003},
  abstract =      {A goal of statistical language modeling is to learn
                   the joint probability function of sequences of words
                   in a language. This is intrinsically difficult
                   because of the curse of dimensionality: a word
                   sequence on which the model will be tested is likely
                   to be different from all the word sequences seen
                   during training. Traditional but very successful
                   approaches based on n-grams obtain generalization by
                   concatenating very short overlapping sequences seen
                   in the training set. We propose to fight the curse of
                   dimensionality by learning a distributed
                   representation for words which allows each training
                   sentence to inform the model about an exponential
                   number of semantically neighboring sentences. The
                   model learns simultaneously (1) a distributed
                   representation for each word along with (2) the
                   probability function for word sequences, expressed in
                   terms of these representations. Generalization is
                   obtained because a sequence of words that has never
                   been seen before gets high probability if it is},
  issn =          {15324435},
  url =           {http://search.ebscohost.com.ezproxy.ub.gu.se/login.aspx?
                  direct=true&db=buh&AN=11468885&site=ehost-live},
}

@unpublished{Bender:2020aa,
  author =        {Emily M. Bender and Alexander Koller},
  month =         {January 26},
  note =          {OpenReview Preprint, anonymous preprint under review},
  title =         {Climbing towards NLU: On Meaning, Form, and
                   Understanding in the Age of Data},
  year =          {2020},
  abstract =      {The success of the large neural language models on
                   many NLP tasks is exciting. However, we find that
                   that these successes sometimes lead to hype in which
                   these models are being described as "understanding"
                   language or capturing "meaning". In this position
                   paper, we argue that a system trained only on form
                   has a priori no way to learn meaning. In keeping with
                   the ACL 2020 theme of "Taking Stock of Where We've
                   Been and Where We're Going", we argue that a clear
                   understanding of the distinction between form and
                   meaning will help guide the field towards better
                   science around natural language understanding.},
  url =           {https://openreview.net/forum?id=GKTvAcb12b},
}

@article{Barsalou:2008aa,
  author =        {Barsalou, Lawrence W.},
  journal =       {Annual Review of Psychology},
  pages =         {617--645},
  title =         {Grounded cognition},
  volume =        {59},
  year =          {2008},
  abstract =      {Grounded cognition rejects traditional views that
                   cognition is computation on amodal symbols in a
                   modular system, independent of the brain's modal
                   systems for perception, action, and introspection.
                   Instead, grounded cognition proposes that modal
                   simulations, bodily states, and situated action
                   underlie cognition. Accumulating behavioral and
                   neural evidence supporting this view is reviewed from
                   research on perception, memory, knowledge, language,
                   thought, social cognition, and development. Theories
                   of grounded cognition are also reviewed, as are
                   origins of the area and common misperceptions of it.
                   Theoretical, empirical, and methodological issues are
                   raised whose future treatment is likely to affect the
                   growth and impact of grounded cognition.},
  doi =           {10.1146/annurev.psych.59.103006.093639},
  url =           {https://doi.org/10.1146/annurev.psych.59.103006.093639},
}

@inproceedings{Baroni:2014ac,
  address =       {Baltimore, Maryland},
  author =        {Baroni, Marco and Dinu, Georgiana and
                   Kruszewski, Germ{\'a}n},
  booktitle =     {Proceedings of the 52nd Annual Meeting of the
                   Association for Computational Linguistics (Volume 1:
                   Long Papers)},
  month =         jun,
  pages =         {238--247},
  publisher =     {Association for Computational Linguistics},
  title =         {Don{'}t count, predict! A systematic comparison of
                   context-counting vs. context-predicting semantic
                   vectors},
  year =          {2014},
  doi =           {10.3115/v1/P14-1023},
  url =           {https://www.aclweb.org/anthology/P14-1023},
}

@techreport{Baroni:2014ab,
  author =        {Marco Baroni and Gemma Boleda},
  institution =   {University of Texas at Austin},
  month =         {April 10},
  type =          {Lectures notes: CS 388: Natural Language Processing},
  title =         {Distributional Semantics},
  year =          {2014},
  url =           {https://www.cs.utexas.edu/~mooney/cs388/slides/dist-sem-
                  intro-NLP-class-UT.pdf},
}

@article{Baroni:2010ab,
  author =        {Baroni, Marco and Lenci, Alessandro},
  journal =       {Computational Linguistics},
  number =        {4},
  pages =         {673--721},
  publisher =     {MIT Press},
  title =         {Distributional memory: A general framework for
                   corpus-based semantics},
  volume =        {36},
  year =          {2010},
  url =           {https://www.mitpressjournals.org/doi/pdfplus/10.1162/
                  coli_a_00016},
}

