{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional semantics\n",
    "\n",
    "Mehdi Ghanimifard, Adam Ek, Wafia Adouane and Simon Dobnik\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on how to work on group assignments.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "---\n",
    "\n",
    "In this lab we will look how to build distributional semantic models from corpora and use semantic similarity captured by these models to do some simple semantic tasks. We are going to use the code that we discussed in the class last time.\n",
    "\n",
    "The following command simply imports all the methods from that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of text which will contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus which you can download from [here](https://gubox.app.box.com/s/7yt07j0cgpnolvbb07n02s0303ski8wr/folder/75208243314?fbclid=IwAR1muU7cL8ZuxRNvXE7VnEuXl5wh3LdmuttdtkrmiyZ9au-fOtLtvR-Ua_c) (Linux and Mac) or [here](https://gubox.app.box.com/s/7yt07j0cgpnolvbb07n02s0303ski8wr/folder/75208243314?fbclid=IwAR1muU7cL8ZuxRNvXE7VnEuXl5wh3LdmuttdtkrmiyZ9au-fOtLtvR-Ua_c) (Windows). (This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).) When unpacked the file is 151mb hence if you are using the lab computers you should store it in a temporary folder outside your home and adjust `corpus_dir` path below.\n",
    "<It may already exist in `/opt/mlt/courses/cl2015/a5`.>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = \"../wikipedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building a count-based model\n",
    "\n",
    "Now you are ready to build a count-based model. The functions for building word spaces can be found in `dist_erk.py`. We will build a model that create count-based vectors for 1000 words. Using the methods from the code imported above build three word matrices with 1000 dimensions as follows: (i) with raw counts (saved to a variable `space_1k`); (ii) with PPMI (`ppmispace_1k`); and (iii) with reduced dimensions SVD (`svdspace_1k`). For the latter use `svddim=5`. **[5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia.txt\n",
      "create count matrices\n",
      "reading file wikipedia.txt\n",
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "word_to_keep = 1000\n",
    "num_dims = 1000\n",
    "svddim = 5\n",
    "\n",
    "#word_to_keep = 10000\n",
    "#num_dims = 1000\n",
    "#svddim = 2\n",
    "\n",
    "\n",
    "# which words to use as targets and context words?\n",
    "ktw = do_word_count(corpus_dir, num_dims)\n",
    "\n",
    "wi = make_word_index(ktw) # word index\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w])  # sorted words # won't be used in the beginning\n",
    "\n",
    "print('create count matrices')\n",
    "space_1k = make_space(corpus_dir, wi, num_dims)\n",
    "print('ppmi transform')\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "print('svd transform')\n",
    "svdspace_1k = svd_transform(ppmispace_1k, num_dims, svddim)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bulding a word2vec model\n",
    "\n",
    "We will also build a continuous-bag-of-words (CBOW) word2vec model using gensim (https://radimrehurek.com/gensim/index.html). Build a CBOW word2vec model, where each word have 300 dimensions and limit the vocabulary size to the most common 1000 words. **[5 marks]**\n",
    "\n",
    "Documentation for the Word2Vec class can be found here: https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# gensim require a iterable class to process the corpus\n",
    "class CorpusReader():\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.corpus_path):\n",
    "            sentence = utils.simple_preprocess(line)\n",
    "            if sentence:\n",
    "                yield sentence \n",
    "\n",
    "corpus = CorpusReader(corpus_dir+'/wikipedia.txt')\n",
    "w2v_model = Word2Vec(sentences=corpus, size=300, max_final_vocab=1000)  \n",
    "w2v_model_150_2000 = Word2Vec(sentences=corpus, size=150, max_final_vocab=2000)\n",
    "                     # training options goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105  567  962  631  443  185  311  189  131   28   93  169\n",
      "   81  125  151  408  194   90   79   29  217  184   62   15   31   70\n",
      "   10    1   41   21    1   31   37    1   30    5   25    7    3   20\n",
      "   11    1   32   36    2    5   66    4    0   46    8   18   28    0\n",
      "   20    7    8   16   10   40    0  175   10    2    7   19    1  174\n",
      "   11    3    1    6    0    0    0   10    9   11    7   24    4    4\n",
      "   14   23   58    7    0   10    2    3   10    6   18    6   13    3\n",
      "   22    0    3    5    3    7   14    3   40   20   19   15    6    8\n",
      "   24    4    5    1   19    0    3    1    0   14    0   14   53    7\n",
      "    7   11    6    5    5    4   12    6   53    1    1  433    4    0\n",
      "    5    7    7   12    1    1    3    4   17    8   16    1    2   31\n",
      "    1   12   14    1   44    6   14    9   38    7    2    6    8    1\n",
      "   10    6   10    1    9    7    9    4    3   10    0   11    3    2\n",
      "    0    2   11   37    2    0    2    1    5    9   10   16   88    6\n",
      "    0   21    1    1    0    2   47    3   27    7    0    2   13    1\n",
      "    2    0    5   31    0    1    0    3   10    0    1    0    3    3\n",
      "   17    1    1   16    3    7    4    7   15    4    0    0    2    5\n",
      "    0    2    0    5    0    9    0    0    8    0   10    0    0    0\n",
      "    2    0    1    3    1    3   15    1    9    0   19   14    0    0\n",
      "    3    2   18    3    1    3    2   19    5    2    4    1   10    6\n",
      "    0    3    3    6    4    2   25    4    6    3    1   25   10   15\n",
      "    3   10   15    1   10    1    8    1   13    1    2    9    9    1\n",
      "    4    1   25    0    4    6    5    5   36    0    2    2    2    0\n",
      "    0    2    3    3    0    1    4    6    5    0   50    2    5    2\n",
      "   14    6    2    2    4    1    9    4    5    3    1    0   12    3\n",
      "    3    2    2    0    0    1    4    7   12    5    0    2    1    2\n",
      "    3    4    7    3    5    0   29    7    1    1    0    3    3    3\n",
      "   10    0   14    2    0    2    4    6    0    5    0    0    1    1\n",
      "    4    1    1    0    0    0    0    3   20    0    0    2    1    5\n",
      "    3    8    3    5    1    2   66    1    2   19    2    1    3    3\n",
      "   21    5    4    2    2    0    4    3    5    0    7    1    6    1\n",
      "    3    3    1    0    3    0    2    0   89    2    3    1    1   14\n",
      "    0    2    1    9    2    3    2    4    2    0   25    0    0   23\n",
      "    0    6    2    1    3    0    2    5    0    4    4    3    0    4\n",
      "   58    3    1    6    2    4    3    3   11    1    1    1   10    0\n",
      "    7    3    1    6    1   18    1    0    4    2    0    8    5    2\n",
      "    0    0    0    0    5    1    2    1    1    3    1    2    1    1\n",
      "    0    6    1    4    1    3   20    1    0    5    2    5    2    1\n",
      "    0    0    0    2    6    1    1    0    1    1    1    0    0    3\n",
      "    3    0    0    6    6   74    3    0   13    5    2    2    1    5\n",
      "    3    3    1    7    4    0    0    2    3    0    4    0    4    1\n",
      "    0    2    5    2    1   14    2    0    0   19    0    1    2    1\n",
      "    0    3    2    0    0    3    1    3    3    2    7   18    7    6\n",
      "    6    0    1    9    1   10    2    0    2    0    2    4    0    0\n",
      "    1    2    0    1    0    2    0    0    0    2    1    2    2    0\n",
      "    3    2    2    0    0    1    2    3    1    1    1    2    0    0\n",
      "    3    0    7    2   39    0   14    0    1    1    0    1    5    3\n",
      "   11    0    3    0    1    1    0    0    1    9    2    1    0   11\n",
      "    1    3    7    0    0    0   32    1    0    0    0    1    1    3\n",
      "    0    9    0    2    0    1    3    2    6    0    3    0    0    2\n",
      "    3    0    1    0    1    4    0    0    1    1    0    0    5   21\n",
      "    2    1    1    3    0    1    7    1    3    4    0    5    3    0\n",
      "    7    2    0    4    2    0    2    1    4    4    0    0    0    5\n",
      "    3    2    2    0    4    0   23    2    2    2    4    0    1    0\n",
      "    4    0    3    5    3    0    8    0    1   16    1    2    2    7\n",
      "    0    0    1   11    1    0    4    0    1    0    1    2    1    5\n",
      "    0   97    0    2    0    3    0    8    1   14    4    9    2    3\n",
      "    1    1    0    3    4    0    5    1    5    2    0    0    0    2\n",
      "    1    2    1    1    1    1   12    0    2    5    1    0    0   13\n",
      "    2    0    0    0    2    2    0    0    3    1    1    1    1    0\n",
      "    1    2    1    0    0    0   10    0    1    0    1    1    1    1\n",
      "    0    1    0    0    3    2    5    0    0    2    1    0   23    0\n",
      "    0    4    0    1    0    0    0    1    1    2    1    0    1    0\n",
      "    0    4    1    0    1    1    5    1    1    0    1    0    0    0\n",
      "    1    0    0    2    2    3    0    1    0    4    3    3    1    4\n",
      "    0    0    0    6    1    2    1    0    5    3    0    0    1    2\n",
      "    0    5    0    0    2    1    1    4   15    0    0    1    1    3\n",
      "    1    0    1    4    1    1    2    8    1    3    0    0    0    0\n",
      "    1    3    2    1    0    1    0    2    0    0    0    0    1    1\n",
      "    0    1    3    7    0    0   42    4    0    1    2    3    1    0\n",
      "    1    3    2    0    0    4    0    0    0    4    2    0    0    8\n",
      "    2    0    1   15    0    0]\n",
      "house: [ 2.60677695e-01  1.91141978e-01  4.40021425e-01 -2.11571738e-01\n",
      "  1.03577173e+00 -4.10201073e-01 -1.57968533e+00  7.01434076e-01\n",
      " -3.01038623e-01 -7.66122282e-01 -1.08065283e+00  7.77411163e-01\n",
      "  4.91116673e-01 -7.39232153e-02  7.94192076e-01 -2.35568415e-02\n",
      "  3.65083516e-01  6.09248757e-01 -3.31346333e-01 -2.63604701e-01\n",
      " -1.37918293e-01 -7.15152264e-01 -5.09682775e-01  6.54757202e-01\n",
      "  4.70840782e-01  4.15418088e-01 -6.11507893e-01 -8.96146834e-01\n",
      "  4.33289707e-01 -7.49499857e-01 -4.59501833e-01  1.50043726e-01\n",
      "  3.99420336e-02  2.87205800e-02 -1.32792330e+00 -4.28275824e-01\n",
      " -2.28220165e-01  2.66165435e-01  3.59533012e-01  1.41790286e-01\n",
      "  1.07645452e+00  8.47847462e-01 -1.77574778e+00  3.97984266e-01\n",
      " -5.68394423e-01 -4.15430337e-01  3.65465164e-01 -1.44601083e+00\n",
      " -2.95189559e-01 -2.03447372e-01 -4.06114221e-01 -6.16068125e-01\n",
      "  1.14003265e+00  3.86902720e-01  1.42183924e+00  3.89803261e-01\n",
      " -2.51066387e-01  4.28544164e-01  1.67211905e-01 -1.16415334e+00\n",
      "  7.55043268e-01 -5.77474296e-01  1.90338686e-01 -5.10466874e-01\n",
      "  3.54529113e-01  6.89750910e-01 -1.18913233e+00 -1.84852183e+00\n",
      " -2.56482840e-01  3.54038954e-01  6.49661362e-01  8.14359009e-01\n",
      " -8.12961042e-01 -2.34988317e-01 -6.40866160e-01  2.84035146e-01\n",
      " -7.72270203e-01 -7.80369401e-01 -1.28817546e+00 -1.40938163e+00\n",
      "  1.30846703e+00 -1.13570072e-01 -4.17695701e-01 -2.84731120e-01\n",
      " -3.46972734e-01 -9.33682919e-01 -6.01381421e-01 -3.47192347e-01\n",
      " -5.18077910e-01  4.01572138e-03  6.07411981e-01  1.15818799e+00\n",
      "  2.48487368e-01  1.00170517e+00  5.06374478e-01  4.23296869e-01\n",
      "  5.67089140e-01  1.18163490e+00 -8.42757702e-01  1.30003735e-01\n",
      "  1.52504981e+00  3.46831381e-01  6.72855854e-01  4.67636138e-02\n",
      "  5.39160848e-01 -1.49263903e-01  7.75915384e-02 -3.77149403e-01\n",
      " -1.35780528e-01  1.10469341e+00 -1.54019758e-01  6.82648346e-02\n",
      "  1.92808025e-02  6.92755938e-01  1.10094094e+00  1.33064973e+00\n",
      "  5.48149943e-01  8.36118937e-01  1.14889368e-02  2.15470657e-01\n",
      "  1.09877491e+00  1.21034920e+00  1.18723881e+00  3.98023486e-01\n",
      " -8.44819844e-02 -1.31198436e-01  6.50635362e-01 -2.83183604e-01\n",
      " -1.90209776e-01  1.75726153e-02 -9.72730041e-01 -3.86310518e-01\n",
      " -3.34186614e-01 -6.25033498e-01  5.11571884e-01  4.61705863e-01\n",
      "  2.97429949e-01  4.11175847e-01 -5.94783783e-01  1.10021973e+00\n",
      " -4.71652597e-01 -9.03949022e-01 -4.28493857e-01 -3.61366838e-01\n",
      "  3.71713787e-02  1.13523209e+00  7.30757177e-01  7.92271018e-01\n",
      " -2.16454491e-01  3.61203738e-02 -1.29106927e+00  7.08769023e-01\n",
      "  1.79031837e+00  2.98245639e-01  9.25618947e-01 -9.15861487e-01\n",
      " -4.67458099e-01  3.32308173e-01  3.40812169e-02  4.94583279e-01\n",
      " -4.74706432e-03 -7.01014638e-01 -8.61434579e-01  6.95436776e-01\n",
      "  6.42733455e-01  1.09928679e+00  4.76666719e-01  7.20116675e-01\n",
      " -2.71244526e-01 -4.20737416e-01 -1.13225162e+00 -1.22483365e-01\n",
      "  8.01793337e-02  3.62879962e-01  6.12816155e-01  2.58477598e-01\n",
      "  3.23699683e-01 -7.55757928e-01 -8.17542197e-04 -1.98976342e-02\n",
      "  6.26867771e-01 -1.81926036e+00 -6.45505250e-01 -3.27802539e-01\n",
      "  3.66698474e-01  5.53671122e-01 -1.45596778e+00 -5.89486301e-01\n",
      "  2.02698320e-01  3.15837234e-01  1.12818822e-01  1.30537677e+00\n",
      " -1.24459542e-01  8.18542778e-01  2.57487118e-01 -5.01598418e-02\n",
      "  7.26005077e-01  6.79943204e-01 -1.46681976e+00  3.64721149e-01\n",
      "  3.66088688e-01  1.69090014e-02 -2.93508172e-01  8.98049772e-01\n",
      " -1.83795869e-01 -3.87288004e-01  1.03876889e+00  2.32864842e-01\n",
      " -2.12516725e-01 -1.23759818e+00 -3.94161433e-01  1.58837229e-01\n",
      " -6.41076490e-02  2.50179559e-01  2.95455247e-01  5.54536819e-01\n",
      "  5.84459186e-01  1.33317336e-01 -1.00733034e-01 -4.65278208e-01\n",
      " -2.19289184e-01  9.37539160e-01 -1.50151774e-01  1.43284220e-02\n",
      "  2.63561428e-01  1.02243371e-01  2.81939536e-01 -4.67115998e-01\n",
      "  9.18011010e-01 -5.45450859e-03  1.02067339e+00 -4.65880752e-01\n",
      "  4.64228481e-01  4.94562894e-01  2.56960988e-01 -4.05723862e-02\n",
      " -1.04646191e-01 -7.50537634e-01  1.88680366e-01 -1.93618107e+00\n",
      " -9.00049806e-01 -1.00702298e+00  4.41501200e-01  4.00141448e-01\n",
      "  1.51052773e+00 -5.84824383e-01  4.71188158e-01  9.08376455e-01\n",
      "  3.36822957e-01 -2.88396478e-01 -4.85292196e-01  4.95322168e-01\n",
      "  1.64489254e-01  6.22372627e-01 -7.26471469e-02 -1.58970550e-01\n",
      "  1.02148987e-01  4.31976914e-01  3.86457682e-01  5.12805521e-01\n",
      " -3.89934331e-01  9.04405713e-01 -1.29648542e+00  7.81297311e-02\n",
      " -3.87507945e-01  2.19935477e-01 -2.72000022e-02 -1.13596737e+00\n",
      " -7.05962777e-01 -9.08751369e-01  5.60561180e-01 -1.71346396e-01\n",
      " -2.64744729e-01 -3.57966423e-01  1.30695570e+00  1.92374766e-01\n",
      " -6.35880470e-01 -2.14953852e+00  4.05050963e-01  8.79528821e-02\n",
      " -4.27632540e-01  5.13579249e-01 -2.33855713e-02  2.69523054e-01\n",
      " -3.93193603e-01  8.28847766e-01  2.05379888e-01 -3.53953421e-01\n",
      "  2.93795556e-01  5.32034993e-01  3.37281942e-01  8.81518960e-01\n",
      "  1.05796599e+00 -1.03449917e+00 -7.32706964e-01  4.52201396e-01\n",
      " -1.43323720e+00 -3.30675840e-01 -5.18915832e-01  7.75741935e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tova/anaconda3/envs/pytorchEnv/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('house:', space_1k['house'])\n",
    "print('house:', w2v_model['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. It took 40 minutes on a laptop. \n",
    "\n",
    "Additionally, we trained a word2vec model on the same data. The vocabulary size is also 10,000 with 300 dimensions for the words, and truncated to 50 dimensions in SVD. It took about 15 minutes on a desktop.\n",
    "\n",
    "We saved all five matrices [here](https://gubox.app.box.com/folder/75208243314) ([alternative/old link](https://linux.dobnik.net/oc/index.php/s/9NTlpOJfPWGS56t/download?path=%2Flab4-distributional-data&files=pretrained.zip)) which you can load as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait..\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Please wait..')\n",
    "ktw_10k       = np.load('../pretrained/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load('../pretrained/raw_wikipediaktw.npy', allow_pickle=True).all()\n",
    "ppmispace_10k = np.load('../pretrained/ppmi_wikipediaktw.npy', allow_pickle=True).all()\n",
    "svdspace_10k  = np.load('../pretrained/svd50_wikipedia10k.npy', allow_pickle=True).all()\n",
    "w2v_space     = np.load('../pretrained/w2v.npy', allow_pickle=True)\n",
    "w2v_svd_space = np.load('../pretrained/w2v_svd.npy', allow_pickle=True)\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector space can be queried as a dictionary $\\texttt{(word_form: vector, ...)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n",
      "house: [-0.22926676  0.84213424  0.41197702  0.8781869   1.2811967  -1.4847856\n",
      "  1.4102424   0.7267851  -0.5590538   0.04928471  1.8261132  -0.4911551\n",
      "  2.6236389  -0.62284136 -1.4621106   1.1592358   1.0392265  -0.07465155\n",
      "  1.0108253   1.1842203  -1.5743443  -1.3098637  -0.04264146 -0.1076067\n",
      "  0.5574365   0.7599903   0.11031609  0.16449381 -0.40311787 -0.68341875\n",
      "  0.48706874 -0.73431605 -0.2089108  -0.10828558 -0.6296254   1.3785347\n",
      " -0.2206072  -1.0867819  -0.2650222  -0.18507054 -1.6295078  -1.0952461\n",
      "  1.2633797   0.29369423 -0.10325834  1.2930017   0.83000755 -0.14103375\n",
      "  1.786327    0.49764258 -2.0428705   0.64002794 -0.3000837   0.03268864\n",
      " -0.0933575   0.76802623 -0.1682042   1.8946133  -0.10339233  0.78187567\n",
      " -0.28241557 -1.0668939   2.4631667   1.0492538   0.10093345  0.5764743\n",
      " -0.24940039 -0.27094615 -0.5501715  -0.07181013  0.830345    0.06051366\n",
      " -0.75200856  0.03423605  0.12481829  0.35145602 -0.5419142   0.62099475\n",
      " -0.05916285 -0.19540095 -1.824948   -0.9549101   0.7577381  -0.38998654\n",
      " -0.991503   -0.30834386  0.3275724   0.43987712 -0.4884273   0.64058024\n",
      " -0.7868578   1.2572296  -2.1773124  -0.9416153   0.3610698   0.4525526\n",
      " -0.74326986  0.17008787  0.13389921  0.5462773   0.22158578  0.96646893\n",
      " -0.7076577   1.0714767   1.0601343  -1.1172956   0.52038205  0.25404415\n",
      "  0.6282369   1.1315012   1.4397461  -1.8375728   0.03305732 -0.730534\n",
      " -0.8909393  -1.0816691  -1.4132217   0.3262256   0.8700905  -0.30414504\n",
      " -1.5354352   0.11356701 -1.1209993  -0.3432277  -0.28034794  0.29500887\n",
      "  0.18721093  0.2440819  -1.7600738  -0.88288015  0.97310877  1.2320257\n",
      " -2.2670896   1.4865086  -0.5700085   1.164806    1.0586189   0.14098446\n",
      " -0.355204   -0.01398894  0.41787666 -0.32915497  0.42694667  2.0801284\n",
      " -2.1506665   1.0133879   1.0759456   0.9264778  -1.2152892  -0.6602847\n",
      "  0.5060731  -0.32094598 -0.28988057 -0.7394317  -2.0207496   0.71306926\n",
      "  1.2402455  -0.24515495 -0.37114933  1.5931996  -0.6360829  -1.4725424\n",
      " -0.11482902 -1.8906887  -0.54170257 -0.55295926  1.4443214  -0.23814447\n",
      " -0.48672342 -0.5407612   0.42850202  0.13757505 -0.02138608 -1.6466916\n",
      " -0.1910228   0.46083194  0.42380738  0.47265923 -1.1858094  -0.6227363\n",
      "  0.54788095 -1.0688363  -0.1841182  -0.08076867 -0.52222496  0.7198857\n",
      "  0.90227884  0.58870053 -0.43280578  1.5541542  -1.097152    1.3715382\n",
      "  0.27046102  1.6903312   0.04087806 -0.69100976  0.9679937  -0.8470927\n",
      " -1.1461766   0.02025647 -1.3694992  -0.44830137 -0.80377305  0.4249456\n",
      " -1.4186532  -0.3481928   0.19753239 -0.8729285  -1.5973473   1.1367882\n",
      " -0.41963372 -0.6037129  -0.33894223 -1.3070797   0.12363753 -0.43385503\n",
      " -1.8637257   1.4995072  -0.5509144   0.5141327  -0.06147667 -0.0131802\n",
      " -0.7573055   1.7510277  -1.0643308  -0.63295656 -1.064991    1.0223176\n",
      " -0.71696764  0.5522713  -0.06373077  0.23441415  0.85623664 -0.56860155\n",
      " -0.11783724 -0.8743623   0.6011444  -0.05577481 -1.9991608   0.7425451\n",
      "  0.37548137  0.56623465 -0.5068199  -0.1673609  -1.3793486  -1.2628069\n",
      " -0.64876103 -0.6869118  -0.25955796  1.4052316  -2.17065     0.79150665\n",
      " -0.39176947  2.504653    0.08094509  0.7164684  -1.5715998  -1.1838112\n",
      "  0.01541841  1.3190931  -0.2541383  -0.96443075  0.26602322 -1.5274041\n",
      " -0.08827151  0.4345104  -0.90708536  0.08691105 -0.25563017  1.8950318\n",
      "  0.44592023 -0.69715744 -0.7586189   0.20796522  0.9038473   0.30558228\n",
      "  0.14890039 -0.10012661 -0.83041894  1.8578987  -0.35939705 -0.21047702\n",
      "  1.1825349   0.21123861 -0.8749253   0.09556534  0.15295361 -1.065538\n",
      " -0.131343    0.6580647  -0.1913644   1.1124527  -0.68274796 -0.8923838\n",
      " -0.15666054  0.40091816 -0.7420015   0.5618306  -0.1739613  -0.14627855]\n"
     ]
    }
   ],
   "source": [
    "print('house:', space_10k['house'])\n",
    "print('house:', w2v_space['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on word vectors to derive meaning predictions. For example, we can subtract the normalised vectors for `king` minus `queen` and add the resulting vector to `man` and we hope to get the vector for `woman`. Why? **[3 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector that connects king to man should be similar to the vector that connects queen to woman. This can be \n",
    "# illustrated by re-organizing the formula king - queen + man = woman as: king + man = queen + woman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some helpful code that allow us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tova/anaconda3/envs/pytorchEnv/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def make_w2v_dict(wtv):\n",
    "    wtv_dict = dict()\n",
    "    for item in wtv.wv.vocab:\n",
    "        wtv_dict[item]=wtv[item]\n",
    "    return wtv_dict\n",
    "\n",
    "w2v_model_dict = make_w2v_dict(w2v_model)\n",
    "w2v_model_150_2000_dict = make_w2v_dict(w2v_model_150_2000)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity functions\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Compare the count-based method with the word2vec method and comment on the results you get. **[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_space:\n",
      "w2v_svd_space:\n",
      "[('coffee', 0.824350657374129), ('rice', 0.6517502165361037), ('breakfast', 0.6425617646579574), ('sugar', 0.6424276619034837), ('maple', 0.621559712616331), ('flour', 0.6161205360200773), ('tea', 0.6115164157690608), ('chocolate', 0.6062069039110909), ('butter', 0.5933827772008674), ('wheat', 0.5921006396962648)]\n"
     ]
    }
   ],
   "source": [
    "#space_10k:\n",
    "\n",
    "king = normalize(space_10k['king'])\n",
    "queen = normalize(space_10k['queen'])\n",
    "man = normalize(space_10k['man'])\n",
    "woman = normalize(space_10k['woman'])\n",
    "\n",
    "#france = normalize(space_10k['france'])\n",
    "#paris = normalize(space_10k['paris'])\n",
    "#tokyo = normalize(space_10k['tokyo'])\n",
    "#japan = normalize(space_10k['japan'])\n",
    "\n",
    "#coffee_space_10k = normalize(space_10k['coffee'])\n",
    "#black_space_10k = normalize(space_10k['black'])\n",
    "#green_space_10k = normalize(space_10k['green'])\n",
    "#tea_space_10k = normalize(space_10k['tea'])\n",
    "\n",
    "#print(\"space_10k:\")\n",
    "#print(find_similar_to(king - man + woman, space_10k)[:10])\n",
    "#print(find_similar_to(france - paris  + tokyo, space_10k)[:10])\n",
    "#find_similar_to(japan_space_10k - tokyo_space_10k + paris_space_10k, space_10k)[:10]\n",
    "#print(find_similar_to(coffee_space_10k - black_space_10k  + green_space_10k, space_10k)[:10])\n",
    "\n",
    "#ppmispace_10k:\n",
    "\n",
    "king = normalize(ppmispace_10k['king'])\n",
    "queen = normalize(ppmispace_10k['queen'])\n",
    "man = normalize(ppmispace_10k['man'])\n",
    "woman = normalize(ppmispace_10k['woman'])\n",
    "\n",
    "coffee = normalize(ppmispace_10k['coffee'])\n",
    "black = normalize(ppmispace_10k['black'])\n",
    "green = normalize(ppmispace_10k['green'])\n",
    "tea = normalize(ppmispace_10k['tea'])\n",
    "\n",
    "france = normalize(ppmispace_10k['france'])\n",
    "paris = normalize(ppmispace_10k['paris'])\n",
    "tokyo = normalize(ppmispace_10k['tokyo'])\n",
    "japan = normalize(ppmispace_10k['japan'])\n",
    "\n",
    "#print(\"ppmispace_10k:\")\n",
    "#print(find_similar_to(king_ppmispace_10k - man_ppmispace_10k + woman_ppmispace_10k, ppmispace_10k)[:10])\n",
    "#print(find_similar_to(france - paris  + tokyo, ppmispace_10k)[:10])\n",
    "#find_similar_to(japan_ppmispace_10k - tokyo_ppmispace_10k + paris_ppmispace_10k, ppmispace_10k)[:10]\n",
    "#print(find_similar_to(coffee_ppmispace_10k - black_ppmispace_10k  + green_ppmispace_10k, ppmispace_10k)[:10])\n",
    "\n",
    "#svdspace_10k:\n",
    "\n",
    "king = normalize(svdspace_10k['king'])\n",
    "queen = normalize(svdspace_10k['queen'])\n",
    "man = normalize(svdspace_10k['man'])\n",
    "woman = normalize(svdspace_10k['woman'])\n",
    "\n",
    "coffee = normalize(svdspace_10k['coffee'])\n",
    "black = normalize(svdspace_10k['black'])\n",
    "green = normalize(svdspace_10k['green'])\n",
    "tea = normalize(svdspace_10k['tea'])\n",
    "\n",
    "france = normalize(svdspace_10k['france'])\n",
    "paris = normalize(svdspace_10k['paris'])\n",
    "tokyok = normalize(svdspace_10k['tokyo'])\n",
    "japan = normalize(svdspace_10k['japan'])\n",
    "\n",
    "#print(\"svdspace_10k:\")\n",
    "#print(find_similar_to(king_svdspace_10k - man_svdspace_10k + woman_svdspace_10k, svdspace_10k)[:10])\n",
    "#find_similar_to(france - paris  + tokyo, svdspace_10k)[:10]\n",
    "#find_similar_to(japan_svdspace_10k - tokyo_svdspace_10k + paris_svdspace_10k, svdspace_10k)[:10]\n",
    "#print(find_similar_to(coffee_svdspace_10k - black_svdspace_10k  + green_svdspace_10k, svdspace_10k)[:10])\n",
    "\n",
    "#w2v_space:\n",
    "king = normalize(w2v_space['king'])\n",
    "queen = normalize(w2v_space['queen'])\n",
    "man = normalize(w2v_space['man'])\n",
    "woman = normalize(w2v_space['woman'])\n",
    "\n",
    "france = normalize(w2v_space['france'])\n",
    "paris = normalize(w2v_space['paris'])\n",
    "tokyo = normalize(w2v_space['tokyo'])  \n",
    "japan = normalize(w2v_space['japan'])\n",
    "\n",
    "coffee = normalize(w2v_space['coffee'])\n",
    "black = normalize(w2v_space['black'])\n",
    "green = normalize(w2v_space['green'])\n",
    "tea = normalize(w2v_space['tea'])\n",
    "\n",
    "print(\"w2v_space:\")\n",
    "#print(find_similar_to(king - man + woman, w2v_space)[:10])\n",
    "find_similar_to(france - paris  + tokyo, w2v_space)[:10]\n",
    "#find_similar_to(japan - tokyo + paris, w2v_space)[:10]\n",
    "find_similar_to(coffee - black  + green, w2v_space)[:10]\n",
    "#print(find_similar_to(tea - green + black, w2v_space)[:10])\n",
    "\n",
    "\n",
    "#w2v_svd_space:\n",
    "king = normalize(w2v_svd_space['king'])\n",
    "queen = normalize(w2v_svd_space['queen'])\n",
    "man = normalize(w2v_svd_space['man'])\n",
    "woman = normalize(w2v_svd_space['woman'])\n",
    "\n",
    "france = normalize(w2v_svd_space['france'])\n",
    "paris = normalize(w2v_svd_space['paris'])\n",
    "tokyo = normalize(w2v_svd_space['tokyo'])  \n",
    "japan = normalize(w2v_svd_space['japan'])\n",
    "\n",
    "coffee = normalize(w2v_svd_space['coffee'])\n",
    "black = normalize(w2v_svd_space['black'])\n",
    "green = normalize(w2v_svd_space['green'])\n",
    "tea = normalize(w2v_svd_space['tea'])\n",
    "\n",
    "print(\"w2v_svd_space:\")\n",
    "#print(find_similar_to(king_w2v_svd_space - man_w2v_svd_space + woman_w2v_svd_space, w2v_svd_space)[:10])\n",
    "find_similar_to(france - paris  + tokyo, w2v_svd_space)[:10]\n",
    "#find_similar_to(japan_w2v_svd_space - tokyo_w2v_svd_space+ paris_w2v_svd_space, w2v_svd_space)[:10]\n",
    "print(find_similar_to(coffee - black  + green, w2v_svd_space)[:10])\n",
    "#print(find_similar_to(tea_w2v_svd_space - green_w2v_svd_space + black_w2v_svd_space, w2v_svd_space)[:10])\n",
    "\n",
    "###HOME-MADE MODELS FROM 2.1 AND 2.2###\n",
    "\n",
    "#w2v_model (as dictionary):\n",
    "\n",
    "#king = normalize(w2v_model_dict['king'])\n",
    "#queen = normalize(w2v_model_dict['queen'])\n",
    "#man = normalize(w2v_model_dict['man'])\n",
    "#woman = normalize(w2v_model_dict['woman'])\n",
    "\n",
    "#coffee = normalize(w2v_model_dict['coffee'])\n",
    "#black = normalize(w2v_model_dict['black'])\n",
    "#green = normalize(w2v_model_dict['green'])\n",
    "#tea = normalize(w2v_model_dict['tea'])\n",
    "\n",
    "\n",
    "#print(\"w2v_model (as dictionary):\")\n",
    "#print(find_similar_to(king - man + woman, w2v_model_dict)[:10])\n",
    "#print(find_similar_to(france - paris  + tokyo, w2v_model_dict)[:10])\n",
    "#print(find_similar_to(coffee - black + green, w2v_model_dict)[:10])\n",
    "\n",
    "#w2v_model_150_2000 (as dictionary):\n",
    "\n",
    "king = normalize(w2v_model_150_2000_dict['king'])\n",
    "queen = normalize(w2v_model_150_2000_dict['queen'])\n",
    "man = normalize(w2v_model_150_2000_dict['man'])\n",
    "woman = normalize(w2v_model_150_2000_dict['woman'])\n",
    "\n",
    "#france = normalize(w2v_model_150_2000_dict['france'])\n",
    "#paris = normalize(w2v_model_150_2000_dict['paris'])\n",
    "#tokyo = normalize(w2v_model_150_2000_dict['tokyo'])  \n",
    "#japan = normalize(w2v_model_150_2000_dict['japan'])\n",
    "\n",
    "#coffee = normalize(w2v_model_150_2000_dict['coffee'])\n",
    "#black = normalize(w2v_model_150_2000_dict['black'])\n",
    "#green = normalize(w2v_model_150_2000_dict['green'])\n",
    "#tea = normalize(w2v_model_150_2000_dict['tea'])\n",
    "\n",
    "#print(\"w2v_model_150_2000:\")\n",
    "#print(find_similar_to(king - man + woman, w2v_model_150_2000_dict)[:10])\n",
    "#print(find_similar_to(france - paris  + tokyo, w2v_model_150_2000_dict)[:10])\n",
    "#print(find_similar_to(coffee - black + green, w2v_model_150_2000_dict)[:10])\n",
    "\n",
    "#space_1k (doesn't have a big enough vocabulary):\n",
    "\n",
    "#king = normalize(space_1k['king'])\n",
    "#queen = normalize(space_1k['queen'])\n",
    "#man = normalize(space_1k['man'])\n",
    "#woman = normalize(space_1k['woman'])\n",
    "\n",
    "#coffee_space_1k = normalize(space_1k['coffee'])\n",
    "#black_space_1k = normalize(space_1k['black'])\n",
    "#green_space_1kl = normalize(space_1k['green'])\n",
    "#tea_space_1k = normalize(space_1k['tea'])\n",
    "\n",
    "\n",
    "#print(\"space_1k:\")\n",
    "#print(find_similar_to(king - man + woman, space_1k)[:10])\n",
    "#print(find_similar_to(coffee_space_1k - black_space_1k  + green_space_1k, space_1k)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 2 similar pairs of pairs of words and test them using your own models and the pretrained models. Does the resulting vector similarity confirm your expectations? But remember you can only do this if the words are contained in our vector space. **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**<br>\n",
    "It turns out that the closest vector to *king - man + woman* using w2v is actually ...king. The pretrained models perform similarly, all of them giving 'king' as the top candidate and ‘queen’ as second or third candidate. For the formula *France - Paris + Tokyo*, the likeliest candidate predicated by the model is also one of the input words themselves: Tokyo (Japan only shows up as third likeliest candidate, just like queen in the previous example). The same is also true for *coffee - black + green*, where you might expect tea as closest vector, but once again find it at third spot, preceded by the input words themselves: coffee and green. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the number of dimensions, and the window size in the models you built in (2.1) and (2.2). Test them on the examples you found in the previous cell and comment on the new results you get in comparison to the first results you obtained. [**4 marks**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**<br>\n",
    "Having half as many dimensions (150) and twice as many words (2000) didn’t help that much in the w2v model, although it did improve the prediction scores and third number candidate somewhat (from prince to princess). I could not run my secondary examples on the count-based models without getting error messages (ValueError: “operands could not be broadcast together with shapes (50,) (300,)”). Space_1k was too small to even work on the king example, also when increasing the vocabulary to 10 000. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` (a copy is included with this notebook or you can download it from [here](https://linux.dobnik.net/oc/index.php/s/9NTlpOJfPWGS56t/download?path=%2Flab4-distributional-data&files=similarity_judgements.txt.zip)) contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected in on online crowd-sourcing data collection using Mechanical Turk as described in [1]. The score range from 1 (highly dissimilar) to 5 (highly similar).\n",
    "\n",
    "The following code will import them into python lists below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 10\n",
      "number of available word pairs to test: 13\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # it will check if both words from each pair exist in the word matrix.\n",
    "        if w1 in space_1k and w2 in space_1k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "         \n",
    "print(\"number of available words to test:\", len(test_vocab-(test_vocab-set(ktw))))\n",
    "print(\"number of available word pairs to test:\", len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test how the cosine similarity between vectors of each of the five spaces compares with the human judgements on the words collected in the previous step. Which of the five spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores we can use [Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better. The $p$-values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate Spearman's correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.8136\n",
      "p-value = 0.0007\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw similiarities: [0.6053463144588419, 0.7554570233618889, 0.9711167809919755, 0.9315826357027838, 0.9754208978693972, 0.868098303658973, 0.8808572066298728, 0.9702039222697523, 0.975056028371703, 0.9647852216476699, 0.8815865237242774, 0.9644596411599679, 0.9360582834159354]\n",
      "PPMI similiarities: [0.015787171731331283, 0.1523941240586544, 0.21848779165946428, 0.29048023105113163, 0.20372332657803263, 0.0800747896953036, 0.24604982900042865, 0.2896844116975526, 0.3551417142599678, 0.1711784986033382, 0.18422229124535672, 0.2062600304036366, 0.1457266300462318]\n",
      "SVD similiarities: [0.30258693983005214, 0.6894464221766998, 0.6510528826328721, 0.9158170261550581, 0.9014647733164831, 0.8875770686708406, 0.9730764354280005, 0.9442058336875112, 0.8599389619220836, 0.7538711515095899, 0.7247901720974329, 0.9672977601866538, 0.6744171266958809]\n",
      "w2v_similiarities: [-0.06559332713395827, -0.1368602392923372, 0.06293609992224168, 0.32245057111325975, 0.25063532635701413, -0.09251503236998528, 0.11557714805616717, 0.14762887911779887, 0.43254876467791886, -0.07242052741289075, 0.12597953885067267, 0.26511968512387907, -0.05505855438995014]\n",
      "w2v_svd_similiarities: [-0.1869317424104697, -0.08408826530559958, 0.15901631309670838, 0.441342683424421, 0.43008351019750696, -0.1386139073037919, 0.3013986523631261, 0.2521433357540774, 0.5483626620541802, -0.031175644939619385, 0.28590581907496976, 0.49441260278075516, 0.026812285707079882]\n"
     ]
    }
   ],
   "source": [
    "raw_similarities  = [cosine(w1, w2, space_1k) for w1, w2 in word_pairs]\n",
    "print(\"raw similiarities:\", raw_similarities)\n",
    "ppmi_similarities = [cosine(w1, w2, ppmispace_1k) for w1, w2 in word_pairs]\n",
    "print(\"PPMI similiarities:\", ppmi_similarities)\n",
    "svd_similarities  = [cosine(w1, w2, svdspace_1k) for w1, w2 in word_pairs]\n",
    "print(\"SVD similiarities:\", svd_similarities)\n",
    "w2v_similarities = [cosine(w1, w2, w2v_model) for w1, w2 in word_pairs]\n",
    "print(\"w2v_similiarities:\", w2v_similarities)\n",
    "w2v_svd_similarities = [cosine(w1, w2, w2v_svd_space) for w1, w2 in word_pairs]\n",
    "print(\"w2v_svd_similiarities:\", w2v_svd_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlate them? Is this expected? **[4 marks + 2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarities vs. Semantic Similarity:\n",
      "rho     = 0.3024\n",
      "p-value = 0.3153\n",
      "PPMI similarities vs. Semantic Similarity:\n",
      "rho     = 0.6920\n",
      "p-value = 0.0088\n",
      "svd similarities vs. Semantic Similarity:\n",
      "rho     = 0.5204\n",
      "p-value = 0.0683\n",
      "w2v similarities vs. Semantic Similarity:\n",
      "rho     = 0.8024\n",
      "p-value = 0.0010\n",
      "w2v svd similarities vs. Semantic Similarity:\n",
      "rho     = 0.6716\n",
      "p-value = 0.0119\n"
     ]
    }
   ],
   "source": [
    "rho, pval = stats.spearmanr(raw_similarities, semantic_similarity)  \n",
    "print(\"\"\"Raw Similarities vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(ppmi_similarities, semantic_similarity)  \n",
    "print(\"\"\"PPMI similarities vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(svd_similarities, semantic_similarity)  \n",
    "print(\"\"\"svd similarities vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(w2v_similarities, semantic_similarity)  \n",
    "print(\"\"\"w2v similarities vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(w2v_svd_similarities, semantic_similarity)  \n",
    "print(\"\"\"w2v svd similarities vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**<br>\n",
    "The similarities of the w2v model best correlate with the real semantic similarity scores (80 %), and the result is statistically significant. I think it's expected given the performance of the count-based models in Q3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[2 marks + 6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Similarities vs. Visual Similarity:\n",
      "rho     = 0.5416\n",
      "p-value = 0.0559\n",
      "PPMI similarities vs. Visual Similarity:\n",
      "rho     = 0.7920\n",
      "p-value = 0.0012\n",
      "svd similarities vs. Visual Similarity:\n",
      "rho     = 0.6814\n",
      "p-value = 0.0103\n",
      "w2v similarities vs. Visual Similarity:\n",
      "rho     = 0.7105\n",
      "p-value = 0.0065\n",
      "w2v svd similarities vs. Visual Similarity:\n",
      "rho     = 0.6989\n",
      "p-value = 0.0079\n"
     ]
    }
   ],
   "source": [
    "rho, pval = stats.spearmanr(raw_similarities, visual_similarity)  \n",
    "print(\"\"\"Raw Similarities vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(ppmi_similarities, visual_similarity)  \n",
    "print(\"\"\"PPMI similarities vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(svd_similarities, visual_similarity)  \n",
    "print(\"\"\"svd similarities vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(w2v_similarities, visual_similarity)  \n",
    "print(\"\"\"w2v similarities vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))\n",
    "\n",
    "rho, pval = stats.spearmanr(w2v_svd_similarities, visual_similarity)  \n",
    "print(\"\"\"w2v svd similarities vs. Visual Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**<br>\n",
    "The similarities of the positive pointwise mutual information model give the best correlation score here. I have no idea why. :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion\n",
    "\n",
    "What are the limitations of our approach in this lab? Suggest three ways in which the results could be improved. **[6 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**<br>\n",
    "\n",
    "1)Increasing the size of the training corpus might improve scores, but training time would also increase.<br>\n",
    "2)Maybe introducing some hardcoded logic into the models would help improving the results in Q3.<br>\n",
    "3)On the practical side of things, it would have been nice with a complete lecture on this subject in order to get a better understanding for the different models and comparisons before starting.<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "[1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23–25 2014 2014. Association for Computational Linguistics.\n",
    "\n",
    "[2] Y. Bengio, R. Ducharme, P. Vincent, & C. Jauvin (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.\n",
    "\n",
    "[3] Levy, Omer, Yoav Goldberg, and Ido Dagan. \"Improving distributional similarity with lessons learned from word embeddings.\" Transactions of the Association for Computational Linguistics 3 (2015): 211-225.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchEnv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
